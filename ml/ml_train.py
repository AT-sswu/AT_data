"""
ì „í†µì  ë¨¸ì‹ ëŸ¬ë‹ ë¶„ë¥˜ê¸° ë¹„êµ
- Random Forest
- SVM
- XGBoost
- KNN
- Decision Tree
"""

import numpy as np
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import (accuracy_score, precision_recall_fscore_support,
                             confusion_matrix, classification_report)
import pandas as pd
import os
import time

plt.rcParams['font.family'] = 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False

# 5ê°œ í´ë˜ìŠ¤
CLASS_NAMES = ['lidar', 'motor', 'driving', 'lidar_driving', 'motor_driving']

print("=" * 70)
print("ì „í†µì  ë¨¸ì‹ ëŸ¬ë‹ ë¶„ë¥˜ê¸° ì„±ëŠ¥ ë¹„êµ")
print("=" * 70)

# ==================== 1. ë°ì´í„° ë¡œë“œ ====================
print("\n[1ë‹¨ê³„] ë°ì´í„° ë¡œë“œ...")

try:
    with open('processed_data_traditional_ml.pkl', 'rb') as f:
        data_dict = pickle.load(f)

    X_train = data_dict['X_train']
    X_val = data_dict['X_val']
    X_test = data_dict['X_test']
    y_train = data_dict['y_train']
    y_val = data_dict['y_val']
    y_test = data_dict['y_test']
    feature_names = data_dict['feature_names']

    # Train + Val í•©ì¹˜ê¸° (ì „í†µì  MLì€ Early Stopping ë¶ˆí•„ìš”)
    X_train_full = np.vstack([X_train, X_val])
    y_train_full = np.concatenate([y_train, y_val])

    print(f"âœ“ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
    print(f"  - Train: {len(X_train_full):,}ê°œ")
    print(f"  - Test: {len(X_test):,}ê°œ")
    print(f"  - íŠ¹ì§• ìˆ˜: {len(feature_names)}ê°œ")

except FileNotFoundError:
    print("ï¸  processed_data_traditional_ml.pklì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print("ë¨¼ì € traditional_ml_data_prep.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”!")
    exit(1)

os.makedirs('results_traditional_ml', exist_ok=True)

# ==================== 2. ë¶„ë¥˜ê¸° ì •ì˜ ====================
print("\n[2ë‹¨ê³„] ë¶„ë¥˜ê¸° ì •ì˜...")

classifiers = {
    'Random Forest': RandomForestClassifier(
        n_estimators=100,
        max_depth=20,
        random_state=42,
        n_jobs=-1
    ),
    'XGBoost': GradientBoostingClassifier(
        n_estimators=100,
        max_depth=5,
        learning_rate=0.1,
        random_state=42
    ),
    'SVM (RBF)': SVC(
        kernel='rbf',
        C=10,
        gamma='scale',
        random_state=42
    ),
    'KNN': KNeighborsClassifier(
        n_neighbors=5,
        n_jobs=-1
    ),
    'Decision Tree': DecisionTreeClassifier(
        max_depth=20,
        random_state=42
    )
}

print(f"âœ“ {len(classifiers)}ê°œ ë¶„ë¥˜ê¸° ì¤€ë¹„ ì™„ë£Œ")

# ==================== 3. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ ====================
print("\n[3ë‹¨ê³„] ëª¨ë¸ í•™ìŠµ ë° í‰ê°€...")

results = {}

for name, clf in classifiers.items():
    print(f"\n{'=' * 60}")
    print(f"[{name}] í•™ìŠµ ì¤‘...")

    # í•™ìŠµ ì‹œì‘
    start_time = time.time()
    clf.fit(X_train_full, y_train_full)
    train_time = time.time() - start_time

    # ì˜ˆì¸¡
    start_time = time.time()
    y_pred = clf.predict(X_test)
    predict_time = time.time() - start_time

    # ì„±ëŠ¥ ì§€í‘œ
    accuracy = accuracy_score(y_test, y_pred)
    precision, recall, f1, _ = precision_recall_fscore_support(
        y_test, y_pred, average='weighted', zero_division=0
    )

    print(f"âœ“ í•™ìŠµ ì™„ë£Œ!")
    print(f"  - í•™ìŠµ ì‹œê°„: {train_time:.2f}ì´ˆ")
    print(f"  - ì˜ˆì¸¡ ì‹œê°„: {predict_time:.4f}ì´ˆ")
    print(f"  - Accuracy: {accuracy:.4f} ({accuracy * 100:.2f}%)")
    print(f"  - Precision: {precision:.4f}")
    print(f"  - Recall: {recall:.4f}")
    print(f"  - F1-Score: {f1:.4f}")

    # ê²°ê³¼ ì €ì¥
    results[name] = {
        'model': clf,
        'y_pred': y_pred,
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'train_time': train_time,
        'predict_time': predict_time
    }

# ==================== 4. ì„±ëŠ¥ ë¹„êµ ì‹œê°í™” ====================
print("\n[4ë‹¨ê³„] ì„±ëŠ¥ ë¹„êµ ì‹œê°í™”...")

# 4-1. ì„±ëŠ¥ ì§€í‘œ ë¹„êµ
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

metrics = ['accuracy', 'precision', 'recall', 'f1']
metric_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score']

for idx, (metric, metric_name) in enumerate(zip(metrics, metric_names)):
    ax = axes[idx // 2, idx % 2]

    model_names = list(results.keys())
    scores = [results[name][metric] for name in model_names]

    bars = ax.barh(model_names, scores, color='steelblue', alpha=0.8, edgecolor='black')

    # ê°’ í‘œì‹œ
    for bar, score in zip(bars, scores):
        ax.text(score + 0.01, bar.get_y() + bar.get_height() / 2,
                f'{score:.4f}',
                va='center', fontsize=11, fontweight='bold')

    ax.set_xlabel(metric_name, fontsize=12, fontweight='bold')
    ax.set_title(f'{metric_name} ë¹„êµ', fontsize=13, fontweight='bold')
    ax.set_xlim([0, 1.1])
    ax.grid(axis='x', alpha=0.3)

plt.tight_layout()
plt.savefig('results_traditional_ml/performance_comparison.png', dpi=300, bbox_inches='tight')
print("âœ“ ì €ì¥: results_traditional_ml/performance_comparison.png")
plt.close()

# 4-2. í•™ìŠµ/ì˜ˆì¸¡ ì‹œê°„ ë¹„êµ
fig, ax = plt.subplots(figsize=(12, 6))

model_names = list(results.keys())
train_times = [results[name]['train_time'] for name in model_names]
predict_times = [results[name]['predict_time'] * 1000 for name in model_names]  # ms

x = np.arange(len(model_names))
width = 0.35

bars1 = ax.bar(x - width / 2, train_times, width, label='í•™ìŠµ ì‹œê°„ (ì´ˆ)',
               color='#FF6B6B', alpha=0.8, edgecolor='black')
bars2 = ax.bar(x + width / 2, predict_times, width, label='ì˜ˆì¸¡ ì‹œê°„ (ms)',
               color='#4ECDC4', alpha=0.8, edgecolor='black')

# ê°’ í‘œì‹œ
for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width() / 2., height,
                f'{height:.2f}',
                ha='center', va='bottom', fontsize=10, fontweight='bold')

ax.set_xlabel('ëª¨ë¸', fontsize=12, fontweight='bold')
ax.set_ylabel('ì‹œê°„', fontsize=12, fontweight='bold')
ax.set_title('ëª¨ë¸ë³„ í•™ìŠµ/ì˜ˆì¸¡ ì‹œê°„ ë¹„êµ', fontsize=14, fontweight='bold')
ax.set_xticks(x)
ax.set_xticklabels(model_names, fontsize=11)
ax.legend(fontsize=11)
ax.grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.savefig('results_traditional_ml/time_comparison.png', dpi=300, bbox_inches='tight')
print("âœ“ ì €ì¥: results_traditional_ml/time_comparison.png")
plt.close()

# 4-3. ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì˜ í˜¼ë™ í–‰ë ¬
best_model_name = max(results, key=lambda x: results[x]['accuracy'])
best_model_pred = results[best_model_name]['y_pred']

cm = confusion_matrix(y_test, best_model_pred)

# 5x5 í˜¼ë™ í–‰ë ¬
plt.figure(figsize=(12, 10))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=CLASS_NAMES,
            yticklabels=CLASS_NAMES,
            cbar_kws={'label': 'ìƒ˜í”Œ ìˆ˜'},
            annot_kws={'size': 11})
plt.xlabel('ì˜ˆì¸¡ í´ë˜ìŠ¤', fontsize=13, fontweight='bold')
plt.ylabel('ì‹¤ì œ í´ë˜ìŠ¤', fontsize=13, fontweight='bold')
plt.title(f'{best_model_name} í˜¼ë™ í–‰ë ¬\nAccuracy: {results[best_model_name]["accuracy"]:.4f}',
          fontsize=15, fontweight='bold', pad=15)
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.savefig('results_traditional_ml/confusion_matrix_best.png', dpi=300, bbox_inches='tight')
print("âœ“ ì €ì¥: results_traditional_ml/confusion_matrix_best.png")
plt.close()

# ==================== 5. íŠ¹ì§• ì¤‘ìš”ë„ ë¶„ì„ (Random Forest) ====================
print("\n[5ë‹¨ê³„] íŠ¹ì§• ì¤‘ìš”ë„ ë¶„ì„ (Random Forest)...")

if 'Random Forest' in results:
    rf_model = results['Random Forest']['model']
    feature_importance = rf_model.feature_importances_

    # ìƒìœ„ 20ê°œ íŠ¹ì§•
    top_indices = np.argsort(feature_importance)[::-1][:20]
    top_features = [feature_names[i] for i in top_indices]
    top_importance = feature_importance[top_indices]

    fig, ax = plt.subplots(figsize=(12, 8))
    ax.barh(range(len(top_features)), top_importance, color='forestgreen', alpha=0.8, edgecolor='black')
    ax.set_yticks(range(len(top_features)))
    ax.set_yticklabels(top_features, fontsize=10)
    ax.set_xlabel('ì¤‘ìš”ë„', fontsize=12, fontweight='bold')
    ax.set_title('Random Forest íŠ¹ì§• ì¤‘ìš”ë„ (ìƒìœ„ 20ê°œ)', fontsize=14, fontweight='bold')
    ax.invert_yaxis()
    ax.grid(axis='x', alpha=0.3)

    plt.tight_layout()
    plt.savefig('results_traditional_ml/feature_importance_rf.png', dpi=300, bbox_inches='tight')
    print("âœ“ ì €ì¥: results_traditional_ml/feature_importance_rf.png")
    plt.close()

# ==================== 6. í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„ ====================
print("\n[6ë‹¨ê³„] í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„...")

print("\n" + "=" * 70)
print(f"[{best_model_name}] í´ë˜ìŠ¤ë³„ ì„¸ë¶€ ì„±ëŠ¥")
print("=" * 70)

print(classification_report(y_test, best_model_pred, target_names=CLASS_NAMES, digits=4))

# í´ë˜ìŠ¤ë³„ ì •í™•ë„ ì‹œê°í™”
class_accuracies = []
for class_id, class_name in enumerate(CLASS_NAMES):
    class_indices = np.where(y_test == class_id)[0]
    if len(class_indices) > 0:
        class_accuracy = np.mean(best_model_pred[class_indices] == class_id)
        class_accuracies.append(class_accuracy)
    else:
        class_accuracies.append(0)

fig, ax = plt.subplots(figsize=(12, 6))
bars = ax.bar(CLASS_NAMES, class_accuracies, color='teal', alpha=0.8, edgecolor='black')

# ê°’ í‘œì‹œ
for bar, acc in zip(bars, class_accuracies):
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width() / 2., height,
            f'{acc:.4f}\n({acc * 100:.2f}%)',
            ha='center', va='bottom', fontsize=11, fontweight='bold')

ax.set_xlabel('í´ë˜ìŠ¤', fontsize=12, fontweight='bold')
ax.set_ylabel('ì •í™•ë„', fontsize=12, fontweight='bold')
ax.set_title(f'{best_model_name} - í´ë˜ìŠ¤ë³„ ì •í™•ë„', fontsize=14, fontweight='bold')
ax.set_ylim([0, 1.1])
ax.grid(axis='y', alpha=0.3)
plt.xticks(rotation=45, ha='right')

plt.tight_layout()
plt.savefig('results_traditional_ml/class_accuracy.png', dpi=300, bbox_inches='tight')
print("âœ“ ì €ì¥: results_traditional_ml/class_accuracy.png")
plt.close()

# ==================== 7. ê²°ê³¼ í…Œì´ë¸” ìƒì„± ====================
print("\n[7ë‹¨ê³„] ê²°ê³¼ ìš”ì•½ í…Œì´ë¸”...")

comparison_df = pd.DataFrame({
    'ëª¨ë¸': list(results.keys()),
    'Accuracy': [f"{results[name]['accuracy']:.4f}" for name in results.keys()],
    'Precision': [f"{results[name]['precision']:.4f}" for name in results.keys()],
    'Recall': [f"{results[name]['recall']:.4f}" for name in results.keys()],
    'F1-Score': [f"{results[name]['f1']:.4f}" for name in results.keys()],
    'í•™ìŠµ ì‹œê°„(ì´ˆ)': [f"{results[name]['train_time']:.2f}" for name in results.keys()],
    'ì˜ˆì¸¡ ì‹œê°„(ms)': [f"{results[name]['predict_time'] * 1000:.2f}" for name in results.keys()]
})

print("\n" + "=" * 100)
print("ğŸ“Š ì „í†µì  ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ")
print("=" * 100)
print(comparison_df.to_string(index=False))

comparison_df.to_csv('results_traditional_ml/model_comparison.csv', index=False, encoding='utf-8-sig')
print("\nâœ“ ì €ì¥: results_traditional_ml/model_comparison.csv")

# ==================== 8. ìµœê³  ëª¨ë¸ ì €ì¥ ====================
print("\n[8ë‹¨ê³„] ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥...")

best_model = results[best_model_name]['model']

with open('results_traditional_ml/best_traditional_model.pkl', 'wb') as f:
    pickle.dump({
        'model': best_model,
        'model_name': best_model_name,
        'accuracy': results[best_model_name]['accuracy'],
        'feature_names': feature_names,
        'class_names': CLASS_NAMES
    }, f)

print(f"âœ“ ì €ì¥: results_traditional_ml/best_traditional_model.pkl")
print(f"  - ìµœê³  ëª¨ë¸: {best_model_name}")
print(f"  - Accuracy: {results[best_model_name]['accuracy']:.4f}")

# ==================== 9. ìµœì¢… ìš”ì•½ ====================
print("\n" + "=" * 70)
print("ì „í†µì  ë¨¸ì‹ ëŸ¬ë‹ ë¶„ë¥˜ê¸° ë¹„êµ ì™„ë£Œ!")
print("=" * 70)

print(f"\n ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_name}")
print(f"  - Accuracy: {results[best_model_name]['accuracy']:.4f}")
print(f"  - F1-Score: {results[best_model_name]['f1']:.4f}")
print(f"  - í•™ìŠµ ì‹œê°„: {results[best_model_name]['train_time']:.2f}ì´ˆ")
print(f"  - ì˜ˆì¸¡ ì‹œê°„: {results[best_model_name]['predict_time'] * 1000:.2f}ms")

print(f"\n ìƒì„±ëœ íŒŒì¼:")
print(f"  - results_traditional_ml/performance_comparison.png")
print(f"  - results_traditional_ml/time_comparison.png")
print(f"  - results_traditional_ml/confusion_matrix_best.png")
print(f"  - results_traditional_ml/feature_importance_rf.png")
print(f"  - results_traditional_ml/class_accuracy.png")
print(f"  - results_traditional_ml/model_comparison.csv")
print(f"  - results_traditional_ml/best_traditional_model.pkl")

print(f"\n í´ë˜ìŠ¤ ì •ë³´:")
print(f"  - ì´ {len(CLASS_NAMES)}ê°œ í´ë˜ìŠ¤")
print(f"  - {', '.join(CLASS_NAMES)}")
