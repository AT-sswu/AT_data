"""
DNN-MLP íšŒê·€ ëª¨ë¸ í•™ìŠµ (5ê°œ í´ë˜ìŠ¤)
íŠ¹ì§• ë²¡í„° â†’ ìµœì  ê³µì§„ ì£¼íŒŒìˆ˜ ì§ì ‘ ì˜ˆì¸¡
"""

import numpy as np
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
import os

plt.rcParams['font.family'] = 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False

# ==================== ì„¤ì • ====================
BATCH_SIZE = 128
EPOCHS = 300  # ì¶©ë¶„í•œ í•™ìŠµ ì‹œê°„ í™•ë³´
LEARNING_RATE = 0.001

# ë¼ë²¨ ë§¤í•‘ (5ê°œ í´ë˜ìŠ¤)
LABEL_MAP = {
    'lidar': 0,
    'motor': 1,
    'driving': 2,
    'lidar_driving': 3,
    'motor_driving': 4
}

# ì—­ë§¤í•‘
REVERSE_LABEL_MAP = {v: k for k, v in LABEL_MAP.items()}

print("=" * 60)
print("DNN-MLP íšŒê·€ ëª¨ë¸ í•™ìŠµ (5ê°œ í´ë˜ìŠ¤)")
print("=" * 60)

# GPU í™•ì¸
gpus = tf.config.list_physical_devices('GPU')
if len(gpus) > 0:
    print(f"\nâœ“ GPU ê°€ì†: {len(gpus)}ê°œ GPU")
else:
    print(f"\nâœ“ CPU ëª¨ë“œ")

# ==================== 1. ë°ì´í„° ë¡œë“œ ====================
print("\n[1ë‹¨ê³„] ë°ì´í„° ë¡œë“œ...")

try:
    with open('processed_data_dnn_regression.pkl', 'rb') as f:
        data_dict = pickle.load(f)

    X_train = data_dict['X_train']
    X_val = data_dict['X_val']
    X_test = data_dict['X_test']
    y_train = data_dict['y_train']
    y_val = data_dict['y_val']
    y_test = data_dict['y_test']
    feature_count = data_dict['feature_count']
    label_map = data_dict.get('label_map', LABEL_MAP)

    print(f"âœ“ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
    print(f"  - Train: {X_train.shape}")
    print(f"  - Val: {X_val.shape}")
    print(f"  - Test: {X_test.shape}")
    print(f"  - íŠ¹ì§• ìˆ˜: {feature_count}ê°œ")
    print(f"  - í´ë˜ìŠ¤ ìˆ˜: {len(label_map)}ê°œ")
    print(f"  - ì£¼íŒŒìˆ˜ ë²”ìœ„: {y_train.min():.2f} ~ {y_train.max():.2f} Hz")

except FileNotFoundError:
    print("ï¸  processed_data_dnn_regression.pklì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print("ë¨¼ì € dnn_mlp_data_prep.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”!")
    exit(1)

# ==================== 2. DNN-MLP íšŒê·€ ëª¨ë¸ êµ¬ì¶• ====================
print("\n[2ë‹¨ê³„] DNN-MLP íšŒê·€ ëª¨ë¸ êµ¬ì¶•...")

model = Sequential([
    # Input Layer
    Dense(256, activation='relu', input_shape=(feature_count,)),
    BatchNormalization(),
    Dropout(0.4),

    # Hidden Layer 1
    Dense(128, activation='relu'),
    BatchNormalization(),
    Dropout(0.4),

    # Hidden Layer 2
    Dense(64, activation='relu'),
    BatchNormalization(),
    Dropout(0.3),

    # Hidden Layer 3
    Dense(32, activation='relu'),
    Dropout(0.3),

    # Hidden Layer 4
    Dense(16, activation='relu'),

    # Output Layer (íšŒê·€)
    Dense(1, activation='linear')
])

model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),
    loss='mean_squared_error',
    metrics=['mae']
)

print("âœ“ ëª¨ë¸ êµ¬ì¶• ì™„ë£Œ (DNN-MLP íšŒê·€)")
model.summary()

# ==================== 3. ì½œë°± ì„¤ì • ====================
print("\n[3ë‹¨ê³„] ì½œë°± ì„¤ì •...")

os.makedirs('results_dnn', exist_ok=True)

callbacks = [
    EarlyStopping(
        monitor='val_loss',
        patience=100,  # ì¸ë‚´ì‹¬ì„ 2ë°°ë¡œ ëŠ˜ë¦¼ (15 â†’ 30)
        restore_best_weights=True,
        verbose=1
    ),

    ModelCheckpoint(
        'results_dnn/best_dnn_regression_model.keras',
        monitor='val_mae',
        save_best_only=True,
        verbose=1
    ),

    ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=12,  # í•™ìŠµë¥  ê°ì†Œë„ ë” ëŠ¦ê²Œ (7 â†’ 12)
        min_lr=1e-8,  # ìµœì†Œ í•™ìŠµë¥ ë„ ë” ì‘ê²Œ
        verbose=1
    )
]

print("âœ“ ì½œë°± ì„¤ì • ì™„ë£Œ")

# ==================== 4. ëª¨ë¸ í•™ìŠµ ====================
print("\n[4ë‹¨ê³„] ëª¨ë¸ í•™ìŠµ...")
print(f"  - Batch Size: {BATCH_SIZE}")
print(f"  - Max Epochs: {EPOCHS}")
print(f"  - Learning Rate: {LEARNING_RATE}")
print("\n" + "=" * 60)

history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    callbacks=callbacks,
    verbose=1
)

print("\n" + "=" * 60)
print("âœ“ í•™ìŠµ ì™„ë£Œ!")

# ==================== 5. í•™ìŠµ ê³¼ì • ì‹œê°í™” ====================
print("\n[5ë‹¨ê³„] í•™ìŠµ ê³¼ì • ì‹œê°í™”...")

fig, axes = plt.subplots(1, 2, figsize=(15, 5))

# Loss (MSE)
axes[0].plot(history.history['loss'], label='Train Loss', linewidth=2)
axes[0].plot(history.history['val_loss'], label='Val Loss', linewidth=2)
axes[0].set_xlabel('Epoch', fontsize=12)
axes[0].set_ylabel('MSE Loss', fontsize=12)
axes[0].set_title('DNN-MLP Loss ë³€í™” (MSE)', fontsize=14, fontweight='bold')
axes[0].legend(fontsize=10)
axes[0].grid(True, alpha=0.3)

# MAE
axes[1].plot(history.history['mae'], label='Train MAE', linewidth=2)
axes[1].plot(history.history['val_mae'], label='Val MAE', linewidth=2)
axes[1].set_xlabel('Epoch', fontsize=12)
axes[1].set_ylabel('MAE (Hz)', fontsize=12)
axes[1].set_title('DNN-MLP í‰ê·  ì ˆëŒ€ ì˜¤ì°¨', fontsize=14, fontweight='bold')
axes[1].legend(fontsize=10)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('results_dnn/dnn_regression_training_history.png', dpi=300, bbox_inches='tight')
print("âœ“ ì €ì¥: results_dnn/dnn_regression_training_history.png")
plt.close()

# ==================== 6. Test í‰ê°€ ====================
print("\n[6ë‹¨ê³„] Test ë°ì´í„° í‰ê°€...")

best_model = keras.models.load_model('results_dnn/best_dnn_regression_model.keras')

y_pred = best_model.predict(X_test, verbose=0).flatten()

# ì„±ëŠ¥ ì§€í‘œ
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print(f"\nâœ“ Test ì„±ëŠ¥:")
print(f"  - MAE: {mae:.4f} Hz")
print(f"  - RMSE: {rmse:.4f} Hz")
print(f"  - RÂ² Score: {r2:.4f}")

# ì˜¤ì°¨ ë¶„ì„
errors = np.abs(y_test - y_pred)
print(f"\nâœ“ ì˜¤ì°¨ í†µê³„:")
print(f"  - ìµœì†Œ ì˜¤ì°¨: {errors.min():.4f} Hz")
print(f"  - ìµœëŒ€ ì˜¤ì°¨: {errors.max():.4f} Hz")
print(f"  - ì¤‘ì•™ê°’ ì˜¤ì°¨: {np.median(errors):.4f} Hz")
print(f"  - ì˜¤ì°¨ < 1Hz: {np.sum(errors < 1.0) / len(errors) * 100:.2f}%")
print(f"  - ì˜¤ì°¨ < 2Hz: {np.sum(errors < 2.0) / len(errors) * 100:.2f}%")
print(f"  - ì˜¤ì°¨ < 5Hz: {np.sum(errors < 5.0) / len(errors) * 100:.2f}%")

# ==================== 7. ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™” ====================
print("\n[7ë‹¨ê³„] ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”...")

# ì‹¤ì œ vs ì˜ˆì¸¡
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# ì‚°ì ë„
axes[0].scatter(y_test, y_pred, alpha=0.5, s=20)
axes[0].plot([y_test.min(), y_test.max()],
             [y_test.min(), y_test.max()],
             'r--', linewidth=2, label='ì™„ë²½í•œ ì˜ˆì¸¡')
axes[0].set_xlabel('ì‹¤ì œ ì£¼íŒŒìˆ˜ (Hz)', fontsize=12, fontweight='bold')
axes[0].set_ylabel('ì˜ˆì¸¡ ì£¼íŒŒìˆ˜ (Hz)', fontsize=12, fontweight='bold')
axes[0].set_title(f'DNN-MLP ì‹¤ì œ vs ì˜ˆì¸¡\n(MAE: {mae:.2f} Hz, RÂ²: {r2:.3f})',
                  fontsize=13, fontweight='bold')
axes[0].legend(fontsize=10)
axes[0].grid(True, alpha=0.3)

# ì˜¤ì°¨ íˆìŠ¤í† ê·¸ë¨
axes[1].hist(errors, bins=50, color='orange', alpha=0.7, edgecolor='black')
axes[1].axvline(mae, color='red', linestyle='--', linewidth=2,
                label=f'MAE: {mae:.2f} Hz')
axes[1].set_xlabel('ì˜ˆì¸¡ ì˜¤ì°¨ (Hz)', fontsize=12, fontweight='bold')
axes[1].set_ylabel('ìƒ˜í”Œ ìˆ˜', fontsize=12, fontweight='bold')
axes[1].set_title('DNN-MLP ì˜ˆì¸¡ ì˜¤ì°¨ ë¶„í¬', fontsize=13, fontweight='bold')
axes[1].legend(fontsize=10)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('results_dnn/dnn_regression_prediction_analysis.png', dpi=300, bbox_inches='tight')
print("âœ“ ì €ì¥: results_dnn/dnn_regression_prediction_analysis.png")
plt.close()

# ==================== 8. ê²°ê³¼ ì €ì¥ ====================
print("\n[8ë‹¨ê³„] ê²°ê³¼ ì €ì¥...")

results = {
    'y_test': y_test,
    'y_pred': y_pred,
    'mae': mae,
    'rmse': rmse,
    'r2': r2,
    'label_map': label_map,
    'num_classes': len(label_map),
    'model_path': 'results_dnn/best_dnn_regression_model.keras'
}

with open('results_dnn/dnn_regression_results.pkl', 'wb') as f:
    pickle.dump(results, f)

print("âœ“ ì €ì¥: results_dnn/dnn_regression_results.pkl")

# ==================== 9. ìµœì¢… ìš”ì•½ ====================
print("\n" + "=" * 60)
print("DNN-MLP íšŒê·€ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
print("=" * 60)

print(f"\nğŸ“Š ìµœì¢… ì„±ëŠ¥:")
print(f"  - MAE: {mae:.4f} Hz")
print(f"  - RMSE: {rmse:.4f} Hz")
print(f"  - RÂ² Score: {r2:.4f}")

print(f"\n í•™ìŠµ í´ë˜ìŠ¤:")
for label_name, label_id in sorted(label_map.items(), key=lambda x: x[1]):
    print(f"  - {label_id}: {label_name}")

print(f"\n ìƒì„±ëœ íŒŒì¼:")
print(f"  - ëª¨ë¸: results_dnn/best_dnn_regression_model.keras")
print(f"  - í•™ìŠµ ê·¸ë˜í”„: results_dnn/dnn_regression_training_history.png")
print(f"  - ì˜ˆì¸¡ ë¶„ì„: results_dnn/dnn_regression_prediction_analysis.png")
print(f"  - ê²°ê³¼ ë°ì´í„°: results_dnn/dnn_regression_results.pkl")

print(f"\n ëª¨ë“  DNN-MLP ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")