import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
import os
import pickle

# ì„¤ì •
DATA_FOLDER = '/Users/seohyeon/PycharmProjects/AT_data/data_v1'
SEQUENCE_LENGTH = 100  # ì‹œí€€ìŠ¤ ê¸¸ì´ (100ê°œ ì—°ì† ë°ì´í„°í¬ì¸íŠ¸)
STEP_SIZE = 10  # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìŠ¤í… (10ê°œì”© ê±´ë„ˆë›°ê¸°)
RANDOM_STATE = 42

# ğŸ”§ ë³€ê²½1: ì‹œë“œ ê³ ì • (ì¬í˜„ì„±)
np.random.seed(RANDOM_STATE)

# ë¼ë²¨ ë§¤í•‘ (5ê°œ í´ë˜ìŠ¤)
LABEL_MAP = {
    'lidar': 0,
    'motor': 1,
    'driving': 2,
    'lidar_driving': 3,
    'motor_driving': 4
}

print("=" * 60)
print("ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ì‹œì‘ (ë°ì´í„° ëˆ„ìˆ˜ í•´ê²° ë²„ì „)")
print("=" * 60)

# ==================== 1. ë°ì´í„° ë¡œë“œ ====================
print("\n[1ë‹¨ê³„] CSV íŒŒì¼ ë¡œë“œ ì¤‘...")

all_data = []
all_labels = []

# ë°ì´í„° í´ë” í™•ì¸
if not os.path.exists(DATA_FOLDER):
    print(f"âŒ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {DATA_FOLDER}")
    print(f"í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}")
    print("\ní•´ê²° ë°©ë²•:")
    print("1. ìŠ¤í¬ë¦½íŠ¸ ìƒë‹¨ì˜ DATA_FOLDER ë³€ìˆ˜ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”")
    print("2. ë˜ëŠ” CSV íŒŒì¼ì„ './data' í´ë”ì— ë„£ìœ¼ì„¸ìš”")
    exit(1)

# CSV íŒŒì¼ ëª©ë¡
csv_files = [f for f in os.listdir(DATA_FOLDER) if f.endswith('.csv')]

if len(csv_files) == 0:
    print(f"âŒ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {DATA_FOLDER}")
    exit(1)

print(f"âœ“ ì°¾ì€ CSV íŒŒì¼ ê°œìˆ˜: {len(csv_files)}ê°œ")

# ê° íŒŒì¼ ë¡œë“œ
for filename in sorted(csv_files):
    filepath = os.path.join(DATA_FOLDER, filename)

    # íŒŒì¼ëª…ì—ì„œ ë¼ë²¨ ì¶”ì¶œ (ìˆœì„œ ì¤‘ìš”: ë³µí•© ë¼ë²¨ì„ ë¨¼ì € ì²´í¬)
    if 'lidar_driving' in filename.lower():
        label = LABEL_MAP['lidar_driving']
        label_name = 'lidar_driving'
    elif 'motor_driving' in filename.lower():
        label = LABEL_MAP['motor_driving']
        label_name = 'motor_driving'
    elif 'lidar' in filename.lower():
        label = LABEL_MAP['lidar']
        label_name = 'lidar'
    elif 'motor' in filename.lower():
        label = LABEL_MAP['motor']
        label_name = 'motor'
    elif 'driving' in filename.lower():
        label = LABEL_MAP['driving']
        label_name = 'driving'
    else:
        print(f"âš ï¸ ë¼ë²¨ì„ ì¸ì‹í•  ìˆ˜ ì—†ëŠ” íŒŒì¼: {filename}")
        continue

    # CSV ë¡œë“œ
    df = pd.read_csv(filepath)

    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ (Time ì œì™¸)
    feature_columns = ['Accel_X', 'Accel_Y', 'Accel_Z', 'Gyro_X', 'Gyro_Y', 'Gyro_Z']

    # ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸
    if not all(col in df.columns for col in feature_columns):
        print(f"âš ï¸ í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ëŠ” íŒŒì¼: {filename}")
        print(f"   íŒŒì¼ ì»¬ëŸ¼: {df.columns.tolist()}")
        continue

    data = df[feature_columns].values

    print(f"  âœ“ {filename}: {len(data):,}ê°œ ìƒ˜í”Œ ({label_name})")

    all_data.append(data)
    all_labels.extend([label] * len(data))

# ë°ì´í„° í†µí•©
X_raw = np.vstack(all_data)
y_raw = np.array(all_labels)

print(f"\nâœ“ ì „ì²´ ë°ì´í„° í˜•íƒœ: {X_raw.shape}")
print(f"âœ“ ì „ì²´ ë¼ë²¨ í˜•íƒœ: {y_raw.shape}")
print(f"âœ“ í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜:")
for label_name, label_id in LABEL_MAP.items():
    count = np.sum(y_raw == label_id)
    print(f"  - {label_name.capitalize()}: {count:,}ê°œ")

# ==================== 2. ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ ì‹œí€€ìŠ¤ ìƒì„± (ì •ê·œí™” ì „!) ====================
print(f"\n[2ë‹¨ê³„] ì‹œí€€ìŠ¤ ìƒì„± ì¤‘ (ê¸¸ì´={SEQUENCE_LENGTH}, ìŠ¤í…={STEP_SIZE})...")

# ğŸ”§ ë³€ê²½2: ì •ê·œí™” ì „ì— ì‹œí€€ìŠ¤ ìƒì„±
def create_sequences(data, labels, seq_length, step_size):
    """ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ ì‹œí€€ìŠ¤ ìƒì„±"""
    sequences = []
    sequence_labels = []

    # ê° í´ë˜ìŠ¤ë³„ë¡œ ì²˜ë¦¬
    for label_id in np.unique(labels):
        # í•´ë‹¹ í´ë˜ìŠ¤ì˜ ë°ì´í„°ë§Œ ì¶”ì¶œ
        class_indices = np.where(labels == label_id)[0]
        class_data = data[class_indices]

        # ìŠ¬ë¼ì´ë”© ìœˆë„ìš°
        for i in range(0, len(class_data) - seq_length + 1, step_size):
            seq = class_data[i:i + seq_length]
            sequences.append(seq)
            sequence_labels.append(label_id)

    return np.array(sequences), np.array(sequence_labels)

X_seq, y_seq = create_sequences(X_raw, y_raw, SEQUENCE_LENGTH, STEP_SIZE)

print(f"âœ“ ìƒì„±ëœ ì‹œí€€ìŠ¤ ê°œìˆ˜: {len(X_seq):,}ê°œ")
print(f"âœ“ ì‹œí€€ìŠ¤ í˜•íƒœ: {X_seq.shape}")
print(f"  - ìƒ˜í”Œ ìˆ˜: {X_seq.shape[0]:,}")
print(f"  - ì‹œí€€ìŠ¤ ê¸¸ì´: {X_seq.shape[1]}")
print(f"  - íŠ¹ì„± ìˆ˜: {X_seq.shape[2]}")

print(f"\nâœ“ í´ë˜ìŠ¤ë³„ ì‹œí€€ìŠ¤ ìˆ˜:")
for label_name, label_id in LABEL_MAP.items():
    count = np.sum(y_seq == label_id)
    print(f"  - {label_name.capitalize()}: {count:,}ê°œ")

# ==================== 3. ë°ì´í„° ë¶„í•  (ì •ê·œí™” ì „!) ====================
print(f"\n[3ë‹¨ê³„] ë°ì´í„° ë¶„í•  ì¤‘ (Train 70%, Val 15%, Test 15%)...")

# ğŸ”§ ë³€ê²½3: ì •ê·œí™” ì „ì— ë¨¼ì € ë¶„í• !
print("\nâš ï¸  [ì¤‘ìš”] ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€:")
print("  â†’ Train/Val/Test ë¶„í•  í›„ Train ë°ì´í„°ë¡œë§Œ ì •ê·œí™”")

# One-hot ì¸ì½”ë”© (5ê°œ í´ë˜ìŠ¤)
y_seq_onehot = np.eye(5)[y_seq]

# Train + Temp ë¶„í•  (70% / 30%)
X_train, X_temp, y_train, y_temp = train_test_split(
    X_seq, y_seq_onehot,
    test_size=0.3,
    random_state=RANDOM_STATE,
    stratify=y_seq,
    shuffle=True  # ëª…ì‹œì ìœ¼ë¡œ ì„ê¸°
)

# Validation + Test ë¶„í•  (15% / 15%)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp,
    test_size=0.5,
    random_state=RANDOM_STATE,
    stratify=np.argmax(y_temp, axis=1),
    shuffle=True
)

print(f"âœ“ Train ì„¸íŠ¸: {X_train.shape[0]:,}ê°œ")
print(f"âœ“ Validation ì„¸íŠ¸: {X_val.shape[0]:,}ê°œ")
print(f"âœ“ Test ì„¸íŠ¸: {X_test.shape[0]:,}ê°œ")

# ==================== 4. ë°ì´í„° ì •ê·œí™” (ë¶„í•  í›„!) ====================
print(f"\n[4ë‹¨ê³„] ë°ì´í„° ì •ê·œí™” ì¤‘...")

# ğŸ”§ ë³€ê²½4: Train ë°ì´í„°ë¡œë§Œ Scaler í•™ìŠµ!
print("\nâœ“ Train ë°ì´í„°ë¡œë§Œ Scaler í•™ìŠµ (ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€)")

# ì‹œí€€ìŠ¤ë¥¼ 2Dë¡œ ë³€í™˜ (ì •ê·œí™”ë¥¼ ìœ„í•´)
n_samples_train = X_train.shape[0]
n_samples_val = X_val.shape[0]
n_samples_test = X_test.shape[0]

X_train_2d = X_train.reshape(-1, X_train.shape[2])
X_val_2d = X_val.reshape(-1, X_val.shape[2])
X_test_2d = X_test.reshape(-1, X_test.shape[2])

# Scaler ìƒì„± ë° Trainìœ¼ë¡œë§Œ í•™ìŠµ
scaler = MinMaxScaler(feature_range=(0, 1))
X_train_normalized = scaler.fit_transform(X_train_2d)  # fit_transform (Train)
X_val_normalized = scaler.transform(X_val_2d)          # transformë§Œ (Val)
X_test_normalized = scaler.transform(X_test_2d)        # transformë§Œ (Test)

# ë‹¤ì‹œ 3Dë¡œ ë³€í™˜
X_train = X_train_normalized.reshape(n_samples_train, SEQUENCE_LENGTH, -1)
X_val = X_val_normalized.reshape(n_samples_val, SEQUENCE_LENGTH, -1)
X_test = X_test_normalized.reshape(n_samples_test, SEQUENCE_LENGTH, -1)

print(f"âœ“ ì •ê·œí™” ì™„ë£Œ: ë²”ìœ„ [0, 1]")
print(f"\n[Train ë°ì´í„°]")
print(f"  - ìµœì†Œê°’: {X_train.min():.4f}")
print(f"  - ìµœëŒ€ê°’: {X_train.max():.4f}")
print(f"  - í‰ê· : {X_train.mean():.4f}")
print(f"\n[Validation ë°ì´í„°]")
print(f"  - ìµœì†Œê°’: {X_val.min():.4f}")
print(f"  - ìµœëŒ€ê°’: {X_val.max():.4f}")
print(f"  - í‰ê· : {X_val.mean():.4f}")
print(f"\n[Test ë°ì´í„°]")
print(f"  - ìµœì†Œê°’: {X_test.min():.4f}")
print(f"  - ìµœëŒ€ê°’: {X_test.max():.4f}")
print(f"  - í‰ê· : {X_test.mean():.4f}")

# ğŸ”§ ë³€ê²½5: ë°ì´í„° ì¤‘ë³µ ì²´í¬
print(f"\n[5ë‹¨ê³„] ë°ì´í„° í’ˆì§ˆ ê²€ì¦...")

# Train-Test ê°„ ì¤‘ë³µ ì²´í¬
print("\nâœ“ Train-Test ì¤‘ë³µ ìƒ˜í”Œ ê²€ì‚¬ ì¤‘...")
train_flat = X_train.reshape(X_train.shape[0], -1)
test_flat = X_test.reshape(X_test.shape[0], -1)

# ìƒ˜í”Œë§í•´ì„œ ì²´í¬ (ì „ì²´ ì²´í¬ëŠ” ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼)
check_size = min(100, len(train_flat), len(test_flat))
duplicates = 0

for i in range(check_size):
    # ê° test ìƒ˜í”Œì´ trainì— ìˆëŠ”ì§€ í™•ì¸
    if np.any(np.all(np.abs(train_flat - test_flat[i]) < 1e-6, axis=1)):
        duplicates += 1

if duplicates > 0:
    print(f"âš ï¸  ê²½ê³ : {duplicates}/{check_size} ì¤‘ë³µ ìƒ˜í”Œ ë°œê²¬")
    print("  â†’ ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ì˜ STEP_SIZEë¥¼ í¬ê²Œ ì¡°ì •í•˜ê±°ë‚˜")
    print("  â†’ ì‹œê³„ì—´ ë¶„í• (TimeSeriesSplit) ì‚¬ìš© ê³ ë ¤")
else:
    print(f"âœ“ ì¤‘ë³µ ì—†ìŒ (ìƒ˜í”Œ {check_size}ê°œ ì²´í¬)")

# í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
print("\nâœ“ ë¶„í•  í›„ í´ë˜ìŠ¤ ë¶„í¬:")
for split_name, y_split in [("Train", y_train), ("Val", y_val), ("Test", y_test)]:
    print(f"\n  [{split_name}]")
    y_labels = np.argmax(y_split, axis=1)
    for label_name, label_id in LABEL_MAP.items():
        count = np.sum(y_labels == label_id)
        percentage = (count / len(y_labels)) * 100
        print(f"    - {label_name:15}: {count:5,}ê°œ ({percentage:5.2f}%)")

# ==================== 6. ë°ì´í„° ì €ì¥ ====================
print(f"\n[6ë‹¨ê³„] ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ ì¤‘...")

# ì €ì¥í•  ë°ì´í„°
data_dict = {
    'X_train': X_train,
    'X_val': X_val,
    'X_test': X_test,
    'y_train': y_train,
    'y_val': y_val,
    'y_test': y_test,
    'scaler': scaler,
    'label_map': LABEL_MAP,
    'config': {
        'sequence_length': SEQUENCE_LENGTH,
        'step_size': STEP_SIZE,
        'random_state': RANDOM_STATE,
        'normalization': 'MinMaxScaler',
        'normalization_range': (0, 1),
        'data_leakage_prevented': True  # ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€ í™•ì¸
    }
}

# í”¼í´ë¡œ ì €ì¥
with open('../processed_data.pkl', 'wb') as f:
    pickle.dump(data_dict, f)

print(f"âœ“ ì €ì¥ ì™„ë£Œ: processed_data.pkl")

# ==================== 7. ë°ì´í„° ìš”ì•½ ====================
print("\n" + "=" * 60)
print("ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ (ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€ ë²„ì „)")
print("=" * 60)

print(f"\nğŸ“Š ìµœì¢… ë°ì´í„°ì…‹ ì •ë³´:")
print(f"  - Train: {X_train.shape}")
print(f"  - Validation: {X_val.shape}")
print(f"  - Test: {X_test.shape}")
print(f"  - ì‹œí€€ìŠ¤ ê¸¸ì´: {SEQUENCE_LENGTH}")
print(f"  - íŠ¹ì„± ìˆ˜: 6 (Accel_X/Y/Z, Gyro_X/Y/Z)")
print(f"  - í´ë˜ìŠ¤ ìˆ˜: 5 (lidar, motor, driving, lidar_driving, motor_driving)")
