import numpy as np
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
import tensorflow as tf
from tensorflow import keras
from keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.regularizers import l2
import os

plt.rcParams['font.family'] = 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False

# ==================== ì„¤ì • (ì¼ë°˜í™” ê°œì„ ) ====================
BATCH_SIZE = 32
EPOCHS = 50
LEARNING_RATE = 0.003

# ë¼ë²¨ ì´ë¦„ (5ê°œ í´ë˜ìŠ¤)
CLASS_NAMES = ['lidar', 'motor', 'driving', 'lidar_driving', 'motor_driving']

print("=" * 60)
print("LSTM ëª¨ë¸ í•™ìŠµ ì‹œì‘ (ì¼ë°˜í™” ê°œì„  ë²„ì „)")
print("=" * 60)

# GPU í™•ì¸
gpus = tf.config.list_physical_devices('GPU')
if len(gpus) > 0:
    print(f"\nâœ“ GPU ê°€ì† í™œì„±í™”: {len(gpus)}ê°œì˜ GPU ì‚¬ìš©")
else:
    print(f"\nâœ“ CPU ëª¨ë“œë¡œ ì‹¤í–‰")

# ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •
np.random.seed(42)
tf.random.set_seed(42)

# ==================== 1. ë°ì´í„° ë¡œë“œ ====================
print("\n[1ë‹¨ê³„] ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ ì¤‘...")

try:
    with open('../processed_data.pkl', 'rb') as f:
        data_dict = pickle.load(f)

    X_train = data_dict['X_train']
    X_val = data_dict['X_val']
    X_test = data_dict['X_test']
    y_train = data_dict['y_train']
    y_val = data_dict['y_val']
    y_test = data_dict['y_test']

    print(f"âœ“ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
    print(f"  - Train: {X_train.shape}")
    print(f"  - Validation: {X_val.shape}")
    print(f"  - Test: {X_test.shape}")

    # ë°ì´í„° ê²€ì¦
    print("\n[ë°ì´í„° ê²€ì¦]")
    train_samples = X_train.reshape(X_train.shape[0], -1)
    test_samples = X_test.reshape(X_test.shape[0], -1)

    check_size = min(100, len(train_samples), len(test_samples))
    duplicates = 0
    for i in range(check_size):
        if np.any(np.all(train_samples[i] == test_samples[:check_size], axis=1)):
            duplicates += 1

    if duplicates > 0:
        print(f"ï¸  ê²½ê³ : Trainê³¼ Test ë°ì´í„° ê°„ {duplicates}/{check_size} ì¤‘ë³µ ìƒ˜í”Œ ë°œê²¬")
    else:
        print(f"âœ“ ë°ì´í„° ì¤‘ë³µ ì—†ìŒ (ìƒ˜í”Œ {check_size}ê°œ ì²´í¬)")

except FileNotFoundError:
    print(" processed_data.pkl íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print("ë¨¼ì € data_preparation.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”!")
    exit(1)

# ë°ì´í„° ì¦ê°• (ë…¸ì´ì¦ˆ ì¶”ê°€)
print("\n[ë°ì´í„° ì¦ê°•]")
print("âœ“ í›ˆë ¨ ë°ì´í„°ì— ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ ì¶”ê°€ (í‘œì¤€í¸ì°¨: 0.01)")
noise = np.random.normal(0, 0.01, X_train.shape)
X_train_augmented = X_train + noise

# ==================== 2. ëª¨ë¸ êµ¬ì¶• (ì¼ë°˜í™” ê°œì„ ) ====================
print("\n[2ë‹¨ê³„] LSTM ëª¨ë¸ êµ¬ì¶• ì¤‘...")

model = Sequential([
    # Input Layer
    LSTM(32, return_sequences=True,
         input_shape=(X_train.shape[1], X_train.shape[2]),
         kernel_regularizer=l2(0.01),
         recurrent_regularizer=l2(0.01)),
    BatchNormalization(),
    Dropout(0.5),

    # Second LSTM Layer
    LSTM(32, return_sequences=False,
         kernel_regularizer=l2(0.01),
         recurrent_regularizer=l2(0.01)),
    BatchNormalization(),
    Dropout(0.5),

    # Dense Layer
    Dense(16, activation='relu', kernel_regularizer=l2(0.01)),
    Dropout(0.4),

    # Output Layer (5ê°œ í´ë˜ìŠ¤)
    Dense(5, activation='softmax')
])

print("\n[ëª¨ë¸ êµ¬ì¡° ë³€ê²½ì‚¬í•­]")
print("  1. LSTM ìœ ë‹›: 64 â†’ 32 (ëª¨ë¸ ìš©ëŸ‰ ê°ì†Œ)")
print("  2. Dense ìœ ë‹›: 32 â†’ 16")
print("  3. Dropout: 0.3 â†’ 0.5/0.4 (ì •ê·œí™” ê°•í™”)")
print("  4. L2 ì •ê·œí™” ì¶”ê°€ (ê°€ì¤‘ì¹˜: 0.01)")
print("  5. BatchNormalization ë ˆì´ì–´ ì¶”ê°€")

# ëª¨ë¸ ì»´íŒŒì¼
model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

print("\nâœ“ ëª¨ë¸ êµ¬ì¶• ì™„ë£Œ")
model.summary()

# ==================== 3. ì½œë°± ì„¤ì • ====================
print("\n[3ë‹¨ê³„] ì½œë°± ì„¤ì • ì¤‘...")

os.makedirs('../results', exist_ok=True)

callbacks = [
    EarlyStopping(
        monitor='val_loss',
        patience=7,
        restore_best_weights=True,
        verbose=1
    ),

    ModelCheckpoint(
        '../results/best_model_improved.keras',
        monitor='val_accuracy',
        save_best_only=True,
        verbose=1
    ),

    ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=3,
        min_lr=1e-6,
        verbose=1
    )
]

print("âœ“ ì½œë°± ì„¤ì • ì™„ë£Œ")

# ==================== 4. ëª¨ë¸ í•™ìŠµ ====================
print("\n[4ë‹¨ê³„] ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
print(f"  - Batch Size: {BATCH_SIZE}")
print(f"  - Max Epochs: {EPOCHS}")
print(f"  - Learning Rate: {LEARNING_RATE}")
print(f"  - ë°ì´í„° ì¦ê°•: ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ ì¶”ê°€")
print("\n" + "=" * 60)

history = model.fit(
    X_train_augmented, y_train,
    validation_data=(X_val, y_val),
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    callbacks=callbacks,
    verbose=1
)

print("\n" + "=" * 60)
print("âœ“ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")

# ==================== 5. í•™ìŠµ ê³¼ì • ì‹œê°í™” ====================
print("\n[5ë‹¨ê³„] í•™ìŠµ ê³¼ì • ì‹œê°í™” ì¤‘...")

fig, axes = plt.subplots(1, 2, figsize=(15, 5))

# Loss ê·¸ë˜í”„
axes[0].plot(history.history['loss'], label='Train Loss', linewidth=2)
axes[0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)
axes[0].set_xlabel('Epoch', fontsize=12)
axes[0].set_ylabel('Loss', fontsize=12)
axes[0].set_title('ëª¨ë¸ Loss ë³€í™” (ì¼ë°˜í™” ê°œì„ )', fontsize=14, fontweight='bold')
axes[0].legend(fontsize=10)
axes[0].grid(True, alpha=0.3)

# Accuracy ê·¸ë˜í”„
axes[1].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)
axes[1].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)
axes[1].set_xlabel('Epoch', fontsize=12)
axes[1].set_ylabel('Accuracy', fontsize=12)
axes[1].set_title('ëª¨ë¸ Accuracy ë³€í™” (ì¼ë°˜í™” ê°œì„ )', fontsize=14, fontweight='bold')
axes[1].legend(fontsize=10)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('../results/training_history_improved.png', dpi=300, bbox_inches='tight')
print("âœ“ ì €ì¥: ../results/training_history_improved.png")
plt.close()

# ==================== 6. Test ë°ì´í„° í‰ê°€ ====================
print("\n[6ë‹¨ê³„] Test ë°ì´í„° í‰ê°€ ì¤‘...")

# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ
best_model = keras.models.load_model('../results/best_model_improved.keras')

# ì˜ˆì¸¡
y_pred_proba = best_model.predict(X_test, verbose=0)
y_pred = np.argmax(y_pred_proba, axis=1)
y_true = np.argmax(y_test, axis=1)

# ì •í™•ë„
test_accuracy = np.mean(y_pred == y_true)
print(f"\nâœ“ Test Accuracy: {test_accuracy:.4f} ({test_accuracy * 100:.2f}%)")

# ê³¼ì í•© ì§„ë‹¨
train_accuracy = history.history['accuracy'][-1]
val_accuracy = history.history['val_accuracy'][-1]
overfit_gap = train_accuracy - test_accuracy

print(f"\n[ê³¼ì í•© ì§„ë‹¨]")
print(f"  - Train Accuracy: {train_accuracy:.4f}")
print(f"  - Validation Accuracy: {val_accuracy:.4f}")
print(f"  - Test Accuracy: {test_accuracy:.4f}")
print(f"  - Train-Test Gap: {overfit_gap:.4f}")

if overfit_gap > 0.1:
    print("  ï¸  ê³¼ì í•© ì˜ì‹¬ (Gap > 0.1)")
elif overfit_gap < 0.01:
    print("  âœ“ ë§¤ìš° ì¢‹ì€ ì¼ë°˜í™” ì„±ëŠ¥")
else:
    print("  âœ“ ì ì ˆí•œ ì¼ë°˜í™” ì„±ëŠ¥")

# Classification Report
print("\n" + "=" * 60)
print("ë¶„ë¥˜ ì„±ëŠ¥ ë¦¬í¬íŠ¸")
print("=" * 60)
report = classification_report(y_true, y_pred, target_names=CLASS_NAMES, digits=4, output_dict=True)
print(classification_report(y_true, y_pred, target_names=CLASS_NAMES, digits=4))

# ==================== 7. ê¸°ì¡´ ë°©ë²• vs LSTM ë¹„êµ ê·¸ë˜í”„ ====================
print("\n[7ë‹¨ê³„] ê¸°ì¡´ ë°©ë²• vs LSTM ë¹„êµ ê·¸ë˜í”„ ìƒì„± ì¤‘...")

# LSTM ì„±ëŠ¥ ì§€í‘œ ì¶”ì¶œ
lstm_accuracy = report['accuracy']
lstm_precision = report['weighted avg']['precision']
lstm_recall = report['weighted avg']['recall']
lstm_f1 = report['weighted avg']['f1-score']

# ê¸°ì¡´ ë°©ë²• ì„±ëŠ¥ (ì´ì§„ ë¶„ë¥˜ ê¸°ì¤€ - ì˜ˆì‹œ ê°’, ì‹¤ì œ ê°’ìœ¼ë¡œ ìˆ˜ì • í•„ìš”)
baseline_accuracy = 0.295  # ì•½ 29.5%
baseline_precision = 0.115  # ì•½ 11.5%
baseline_recall = 0.295
baseline_f1 = 0.165

# ë¹„êµ ë°ì´í„° ìƒì„±
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
baseline_scores = [baseline_accuracy, baseline_precision, baseline_recall, baseline_f1]
lstm_scores = [lstm_accuracy, lstm_precision, lstm_recall, lstm_f1]

# ê·¸ë˜í”„ ìƒì„±
fig, ax = plt.subplots(figsize=(12, 7))

x = np.arange(len(metrics))
width = 0.35

bars1 = ax.bar(x - width/2, baseline_scores, width, label='ê¸°ì¡´ ë°©ë²•',
               color='#FF9999', edgecolor='black', linewidth=1.5, alpha=0.8)
bars2 = ax.bar(x + width/2, lstm_scores, width, label='LSTM ëª¨ë¸',
               color='#66CCCC', edgecolor='black', linewidth=1.5, alpha=0.8)

# ê°’ í‘œì‹œ
for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.3f}',
                ha='center', va='bottom', fontsize=11, fontweight='bold')

ax.set_xlabel('ì„±ëŠ¥ ì§€í‘œ', fontsize=14, fontweight='bold')
ax.set_ylabel('ì ìˆ˜', fontsize=14, fontweight='bold')
ax.set_title('ê¸°ì¡´ ë°©ë²• vs LSTM ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ', fontsize=16, fontweight='bold', pad=20)
ax.set_xticks(x)
ax.set_xticklabels(metrics, fontsize=12)
ax.legend(fontsize=12, loc='upper left')
ax.set_ylim([0, 1.1])
ax.grid(True, axis='y', alpha=0.3, linestyle='--')

# ë°°ê²½ìƒ‰ ì¶”ê°€
ax.set_facecolor('#F8F8F8')
fig.patch.set_facecolor('white')

plt.tight_layout()
plt.savefig('../results/baseline_vs_lstm_comparison.png', dpi=300, bbox_inches='tight')
print("âœ“ ì €ì¥: ../results/baseline_vs_lstm_comparison.png")
plt.close()

# ê°œì„ ìœ¨ ê³„ì‚° ë° ì¶œë ¥
print("\n" + "=" * 60)
print("ê¸°ì¡´ ë°©ë²• ëŒ€ë¹„ LSTM ê°œì„ ìœ¨")
print("=" * 60)

improvements = {
    'Accuracy': (lstm_accuracy - baseline_accuracy) / baseline_accuracy * 100,
    'Precision': (lstm_precision - baseline_precision) / baseline_precision * 100,
    'Recall': (lstm_recall - baseline_recall) / baseline_recall * 100,
    'F1-Score': (lstm_f1 - baseline_f1) / baseline_f1 * 100
}

for metric, improvement in improvements.items():
    print(f"  {metric:12}: {improvement:+7.2f}% ê°œì„ ")

# ==================== 8. í˜¼ë™ í–‰ë ¬ ì‹œê°í™” ====================
print("\n[8ë‹¨ê³„] í˜¼ë™ í–‰ë ¬ ìƒì„± ì¤‘...")

# í˜¼ë™ í–‰ë ¬ ê³„ì‚°
cm = confusion_matrix(y_true, y_pred)

# ì‹œê°í™” (5x5 í–‰ë ¬)
plt.figure(figsize=(12, 10))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=CLASS_NAMES,
            yticklabels=CLASS_NAMES,
            cbar_kws={'label': 'ìƒ˜í”Œ ìˆ˜'},
            annot_kws={'size': 12})
plt.xlabel('ì˜ˆì¸¡ í´ë˜ìŠ¤', fontsize=13, fontweight='bold')
plt.ylabel('ì‹¤ì œ í´ë˜ìŠ¤', fontsize=13, fontweight='bold')
plt.title('í˜¼ë™ í–‰ë ¬ - ì¼ë°˜í™” ê°œì„  ë²„ì „', fontsize=15, fontweight='bold', pad=15)
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.savefig('../results/confusion_matrix_improved.png', dpi=300, bbox_inches='tight')
print("âœ“ ì €ì¥: ../results/confusion_matrix_improved.png")
plt.close()

# ì •ê·œí™”ëœ í˜¼ë™ í–‰ë ¬ (ë¹„ìœ¨)
cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

plt.figure(figsize=(12, 10))
sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Greens',
            xticklabels=CLASS_NAMES,
            yticklabels=CLASS_NAMES,
            cbar_kws={'label': 'ë¹„ìœ¨'},
            annot_kws={'size': 12})
plt.xlabel('ì˜ˆì¸¡ í´ë˜ìŠ¤', fontsize=13, fontweight='bold')
plt.ylabel('ì‹¤ì œ í´ë˜ìŠ¤', fontsize=13, fontweight='bold')
plt.title('ì •ê·œí™”ëœ í˜¼ë™ í–‰ë ¬ - ì¼ë°˜í™” ê°œì„  ë²„ì „', fontsize=15, fontweight='bold', pad=15)
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.savefig('../results/confusion_matrix_normalized_improved.png', dpi=300, bbox_inches='tight')
print("âœ“ ì €ì¥: ../results/confusion_matrix_normalized_improved.png")
plt.close()

# ==================== 9. ì˜ˆì¸¡ ìƒ˜í”Œ í™•ì¸ ====================
print("\n[9ë‹¨ê³„] ì˜ˆì¸¡ ìƒ˜í”Œ í™•ì¸...")

print("\n" + "=" * 60)
print("ì˜ˆì¸¡ ìƒ˜í”Œ (ê° í´ë˜ìŠ¤ë³„ 3ê°œ)")
print("=" * 60)

for class_id, class_name in enumerate(CLASS_NAMES):
    class_indices = np.where(y_true == class_id)[0][:3]

    print(f"\n[{class_name}]")
    for idx in class_indices:
        true_label = CLASS_NAMES[y_true[idx]]
        pred_label = CLASS_NAMES[y_pred[idx]]
        confidence = y_pred_proba[idx][y_pred[idx]] * 100

        status = "âœ“" if true_label == pred_label else "âœ—"
        print(f"  {status} ì‹¤ì œ: {true_label:15} | ì˜ˆì¸¡: {pred_label:15} | ì‹ ë¢°ë„: {confidence:5.2f}%")

# ==================== 10. í´ë˜ìŠ¤ë³„ ì •í™•ë„ ë¶„ì„ ====================
print("\n[10ë‹¨ê³„] í´ë˜ìŠ¤ë³„ ì •í™•ë„ ë¶„ì„...")

print("\n" + "=" * 60)
print("í´ë˜ìŠ¤ë³„ ì„¸ë¶€ ì„±ëŠ¥")
print("=" * 60)

for class_id, class_name in enumerate(CLASS_NAMES):
    class_indices = np.where(y_true == class_id)[0]
    if len(class_indices) > 0:
        class_accuracy = np.mean(y_pred[class_indices] == class_id)
        total_samples = len(class_indices)
        correct_samples = np.sum(y_pred[class_indices] == class_id)
        print(f"  [{class_name:15}] ì •í™•ë„: {class_accuracy:.4f} ({correct_samples}/{total_samples})")

# ==================== 11. ì„±ëŠ¥ ë¹„êµ í…Œì´ë¸” ìƒì„± ====================
print("\n[11ë‹¨ê³„] ì„±ëŠ¥ ë¹„êµ í…Œì´ë¸” ìƒì„±...")

comparison_data = {
    'ë°©ë²•': ['ê¸°ì¡´ ë°©ë²• (ì´ì§„ ë¶„ë¥˜)', 'LSTM ëª¨ë¸ (5-í´ë˜ìŠ¤)'],
    'Accuracy': [f'{baseline_accuracy:.4f}', f'{lstm_accuracy:.4f}'],
    'Precision': [f'{baseline_precision:.4f}', f'{lstm_precision:.4f}'],
    'Recall': [f'{baseline_recall:.4f}', f'{lstm_recall:.4f}'],
    'F1-Score': [f'{baseline_f1:.4f}', f'{lstm_f1:.4f}']
}

import pandas as pd
comparison_df = pd.DataFrame(comparison_data)

print("\n" + "=" * 60)
print("ì„±ëŠ¥ ë¹„êµ í…Œì´ë¸”")
print("=" * 60)
print(comparison_df.to_string(index=False))

# CSVë¡œ ì €ì¥
comparison_df.to_csv('../results/performance_comparison.csv', index=False, encoding='utf-8-sig')
print("\nâœ“ ì €ì¥: ../results/performance_comparison.csv")

# ==================== 12. ìµœì¢… ê²°ê³¼ ìš”ì•½ ====================
print("\n" + "=" * 60)
print("í•™ìŠµ ë° í‰ê°€ ì™„ë£Œ! (ì¼ë°˜í™” ê°œì„  ë²„ì „)")
print("=" * 60)

print(f"\n ìµœì¢… ì„±ëŠ¥:")
print(f"  - Train Accuracy: {train_accuracy:.4f}")
print(f"  - Validation Accuracy: {val_accuracy:.4f}")
print(f"  - Test Accuracy: {test_accuracy:.4f}")
print(f"  - Overfitting Gap: {overfit_gap:.4f}")

print(f"\n ê¸°ì¡´ ë°©ë²• ëŒ€ë¹„:")
print(f"  - Accuracy ê°œì„ : {improvements['Accuracy']:+.2f}%")
print(f"  - Precision ê°œì„ : {improvements['Precision']:+.2f}%")
print(f"  - Recall ê°œì„ : {improvements['Recall']:+.2f}%")
print(f"  - F1-Score ê°œì„ : {improvements['F1-Score']:+.2f}%")

print(f"\n ì €ì¥ëœ íŒŒì¼:")
print(f"  - ëª¨ë¸: ../results/best_model_improved.keras")
print(f"  - í•™ìŠµ ê·¸ë˜í”„: ../results/training_history_improved.png")
print(f"  - ë¹„êµ ê·¸ë˜í”„: ../results/baseline_vs_lstm_comparison.png")
print(f"  - í˜¼ë™ í–‰ë ¬: ../results/confusion_matrix_improved.png")
print(f"  - ì •ê·œí™” í˜¼ë™ í–‰ë ¬: ../results/confusion_matrix_normalized_improved.png")
print(f"  - ì„±ëŠ¥ ë¹„êµ í…Œì´ë¸”: ../results/performance_comparison.csv")

print(f"\nğŸ”§ ì£¼ìš” ë³€ê²½ì‚¬í•­:")
print(f"  1. ë°°ì¹˜ í¬ê¸°: 64 â†’ 32")
print(f"  2. ì—í¬í¬: 100 â†’ 50")
print(f"  3. í•™ìŠµë¥ : 0.001 â†’ 0.01")
print(f"  4. LSTM ìœ ë‹›: 64 â†’ 32")
print(f"  5. Dropout: 0.3 â†’ 0.5")
print(f"  6. L2 ì •ê·œí™” ì¶”ê°€ (0.01)")
print(f"  7. BatchNormalization ì¶”ê°€")
print(f"  8. ë°ì´í„° ì¦ê°• (ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ)")
print(f"  9. Early Stopping patience: 10 â†’ 7")
print(f"  10. ê¸°ì¡´ ë°©ë²•ê³¼ ë¹„êµ ê·¸ë˜í”„ ì¶”ê°€")

print(f"\n ì‘ì—… ì™„ë£Œ")